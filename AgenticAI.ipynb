{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "88079a16",
      "metadata": {
        "id": "88079a16"
      },
      "source": [
        "# Agentic AI"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "105ae079",
      "metadata": {
        "id": "105ae079"
      },
      "source": [
        "## Prerequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64e8d156",
      "metadata": {
        "id": "64e8d156"
      },
      "source": [
        "### Install a Local LLM with Ollama\n",
        "\n",
        "To run this project locally, we will install and use **Ollama**, a lightweight runtime for local large language models.\n",
        "\n",
        "**Download Ollama:**  \n",
        "https://ollama.com/\n",
        "\n",
        "Once installed, you can pull any model you want to run.  \n",
        "Below are a few recommended examples, but you are free to pick any size or model from the Ollama library.\n",
        "\n",
        "ollama pull qwen3:0.6b\n",
        "\n",
        "or\n",
        "\n",
        "ollama pull ibm/granite4:350m\n",
        "\n",
        "or\n",
        "\n",
        "Choose any model you prefer, make sure the model supports tools.\n",
        "Browse available models here:\n",
        "https://ollama.com/library\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42933887",
      "metadata": {
        "id": "42933887"
      },
      "source": [
        "### Python requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3e73d984",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3e73d984",
        "outputId": "54b3cc2e-6560-4d44-dc4d-8a91c7b58683"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.12/dist-packages (1.0.5)\n",
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-4.1.3-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: mcp in /usr/local/lib/python3.12/dist-packages (1.24.0)\n",
            "Collecting langchain-ollama\n",
            "  Downloading langchain_ollama-1.0.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.3.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langgraph) (2.12.3)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.6.0)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-genai<2.0.0,>=1.56.0 (from langchain-google-genai)\n",
            "  Downloading google_genai-1.57.0-py3-none-any.whl.metadata (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.3/53.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core\n",
            "  Downloading langchain_core-1.2.6-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.4.59)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.12.0)\n",
            "Requirement already satisfied: anyio>=4.5 in /usr/local/lib/python3.12/dist-packages (from mcp) (4.12.0)\n",
            "Requirement already satisfied: httpx-sse>=0.4 in /usr/local/lib/python3.12/dist-packages (from mcp) (0.4.3)\n",
            "Requirement already satisfied: httpx>=0.27.1 in /usr/local/lib/python3.12/dist-packages (from mcp) (0.28.1)\n",
            "Requirement already satisfied: jsonschema>=4.20.0 in /usr/local/lib/python3.12/dist-packages (from mcp) (4.25.1)\n",
            "Requirement already satisfied: pydantic-settings>=2.5.2 in /usr/local/lib/python3.12/dist-packages (from mcp) (2.12.0)\n",
            "Requirement already satisfied: pyjwt>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from pyjwt[crypto]>=2.10.1->mcp) (2.10.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.12/dist-packages (from mcp) (0.0.20)\n",
            "Requirement already satisfied: sse-starlette>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from mcp) (3.0.4)\n",
            "Requirement already satisfied: starlette>=0.27 in /usr/local/lib/python3.12/dist-packages (from mcp) (0.50.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from mcp) (0.4.2)\n",
            "Requirement already satisfied: uvicorn>=0.31.1 in /usr/local/lib/python3.12/dist-packages (from mcp) (0.38.0)\n",
            "Collecting ollama<1.0.0,>=0.6.0 (from langchain-ollama)\n",
            "  Downloading ollama-0.6.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio>=4.5->mcp) (3.11)\n",
            "Collecting google-auth<3.0.0,>=2.46.0 (from google-auth[requests]<3.0.0,>=2.46.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai)\n",
            "  Downloading google_auth-2.47.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (2.32.4)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (15.0.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain-google-genai) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.1->mcp) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.1->mcp) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.1->mcp) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp) (0.30.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph) (1.12.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (2.41.4)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings>=2.5.2->mcp) (1.2.1)\n",
            "Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.12/dist-packages (from pyjwt[crypto]>=2.10.1->mcp) (43.0.3)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn>=0.31.1->mcp) (8.3.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->mcp) (2.0.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.46.0->google-auth[requests]<3.0.0,>=2.46.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.46.0->google-auth[requests]<3.0.0,>=2.46.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (2.5.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->mcp) (2.23)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.46.0->google-auth[requests]<3.0.0,>=2.46.0->google-genai<2.0.0,>=1.56.0->langchain-google-genai) (0.6.1)\n",
            "Downloading langchain_google_genai-4.1.3-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.3/66.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-1.2.6-py3-none-any.whl (489 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.1/489.1 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_ollama-1.0.1-py3-none-any.whl (29 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_genai-1.57.0-py3-none-any.whl (713 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m713.3/713.3 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ollama-0.6.1-py3-none-any.whl (14 kB)\n",
            "Downloading google_auth-2.47.0-py3-none-any.whl (234 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m234.9/234.9 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: filetype, google-auth, ollama, langchain-core, google-genai, langchain-ollama, langchain-google-genai\n",
            "  Attempting uninstall: google-auth\n",
            "    Found existing installation: google-auth 2.43.0\n",
            "    Uninstalling google-auth-2.43.0:\n",
            "      Successfully uninstalled google-auth-2.43.0\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 1.2.1\n",
            "    Uninstalling langchain-core-1.2.1:\n",
            "      Successfully uninstalled langchain-core-1.2.1\n",
            "  Attempting uninstall: google-genai\n",
            "    Found existing installation: google-genai 1.55.0\n",
            "    Uninstalling google-genai-1.55.0:\n",
            "      Successfully uninstalled google-genai-1.55.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires google-auth==2.43.0, but you have google-auth 2.47.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed filetype-1.2.0 google-auth-2.47.0 google-genai-1.57.0 langchain-core-1.2.6 langchain-google-genai-4.1.3 langchain-ollama-1.0.1 ollama-0.6.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "980972f3b7a541f8bf623026a0160346"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install langgraph langchain-google-genai langchain-core mcp langchain-ollama"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "182e2139",
      "metadata": {
        "id": "182e2139"
      },
      "source": [
        "## 1. Define FastMCP Tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "b3eeb67c",
      "metadata": {
        "id": "b3eeb67c"
      },
      "outputs": [],
      "source": [
        "from mcp.server.fastmcp import FastMCP\n",
        "import math\n",
        "from color_blocks_state import color_blocks_state, init_goal_for_search\n",
        "from heuristics import init_goal_for_heuristics, advanced_heuristic\n",
        "from search import search\n",
        "# Initialize FastMCP\n",
        "mcp = FastMCP(\"Unified Solver\")\n",
        "\n",
        "@mcp.tool()\n",
        "def solve_search_with_impl(start_blocks: str, goal_blocks: str) -> int:\n",
        "    \"\"\"\n",
        "    Runs my A* implementation and returns the solution cost.\n",
        "    \"\"\"\n",
        "    init_goal_for_heuristics(goal_blocks)\n",
        "    init_goal_for_search(goal_blocks)\n",
        "\n",
        "    start_state = color_blocks_state(start_blocks)\n",
        "    result = search(start_state, advanced_heuristic)\n",
        "\n",
        "    if result is None:\n",
        "        return -1\n",
        "\n",
        "    return result[-1].g\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc32e06d",
      "metadata": {
        "id": "dc32e06d"
      },
      "source": [
        "## 2. LLM + MCP"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c6e9ea7",
      "metadata": {
        "id": "5c6e9ea7"
      },
      "source": [
        "### 2.1. Global instance of our LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "420c2293",
      "metadata": {
        "id": "420c2293"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# SETUP API KEY if using Google Gemini\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyAXL8xkP1LjtDddT9zKsHErTJYfB8vaDrU\"\n",
        "global_llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash-lite\",\n",
        "    temperature=0\n",
        ")\n",
        "# model = \"gemini-2.5-flash\"\n",
        "# model = \"gemini-2.5-flash-lite\"\n",
        "# global_llm = ChatGoogleGenerativeAI(model=model, temperature=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d7d5dee",
      "metadata": {
        "id": "7d7d5dee"
      },
      "source": [
        "### 2.2. Our agent graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "5dba39da",
      "metadata": {
        "id": "5dba39da"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import MessagesState, START, StateGraph\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "from langgraph.checkpoint.memory import MemorySaver # Optional: For saving graph state\n",
        "\n",
        "\n",
        "def create_agent_graph(sys_msg, tools):\n",
        "    \"\"\" Creates a LangGraph StateGraph with the given tools integrated.\"\"\"\n",
        "\n",
        "    llm = global_llm\n",
        "\n",
        "    if tools:\n",
        "        llm_with_tools = llm.bind_tools(tools)\n",
        "    else:\n",
        "        llm_with_tools = llm\n",
        "\n",
        "    # Node\n",
        "    def assistant(state: MessagesState):\n",
        "        return {\n",
        "            \"messages\": [\n",
        "                llm_with_tools.invoke([sys_msg] + state[\"messages\"])\n",
        "            ]\n",
        "        }\n",
        "\n",
        "    # Graph\n",
        "    builder = StateGraph(MessagesState)\n",
        "\n",
        "    # Define the basic graph structure\n",
        "    builder.add_node(\"assistant\", assistant)\n",
        "    builder.add_edge(START, \"assistant\")\n",
        "\n",
        "    if tools:\n",
        "        builder.add_node(\"tools\", ToolNode(tools))\n",
        "        builder.add_conditional_edges(\n",
        "            \"assistant\",\n",
        "            tools_condition,\n",
        "        )\n",
        "        builder.add_edge(\"tools\", \"assistant\")\n",
        "\n",
        "    react_graph = builder.compile()\n",
        "\n",
        "    return react_graph\n",
        "\n",
        "\n",
        "async def run_agent(prompt, tools, sys_msg=\"\"):\n",
        "\n",
        "    sys_msg = SystemMessage(content=sys_msg)\n",
        "\n",
        "    # 3. Create Graph\n",
        "    graph = create_agent_graph(sys_msg, tools)\n",
        "\n",
        "    # 4. Run (using ainvoke for async tools)\n",
        "    config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "    result = await graph.ainvoke({\"messages\": [HumanMessage(content=prompt)]}, config)\n",
        "\n",
        "    last_msg = result[\"messages\"][-1].content\n",
        "\n",
        "    # Extract tool names and outputs\n",
        "    tools_used = []\n",
        "    tools_output = []\n",
        "\n",
        "    # Parsing logic specific to your request\n",
        "    for msg in result[\"messages\"]:\n",
        "        # In LangChain, tool calls are usually in 'tool_calls' attribute of AIMessage\n",
        "        # or 'name' attribute if it is a ToolMessage\n",
        "        if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
        "             for tool_call in msg.tool_calls:\n",
        "                tools_used.append(tool_call['name'])\n",
        "\n",
        "        if msg.type == 'tool':\n",
        "            tools_output.append(content_to_text(msg.content))\n",
        "\n",
        "    return last_msg, tools_used, tools_output\n",
        "\n",
        "def content_to_text(content) -> str:\n",
        "    # 1) Already a string\n",
        "    if isinstance(content, str):\n",
        "        return content\n",
        "\n",
        "    # 2) Gemini/LC sometimes returns a list of parts, like [{'type':'text','text':...}, ...]\n",
        "    if isinstance(content, list):\n",
        "        parts = []\n",
        "        for item in content:\n",
        "            if isinstance(item, dict):\n",
        "                # common structure: {'type': 'text', 'text': '...'}\n",
        "                if item.get(\"type\") == \"text\" and \"text\" in item:\n",
        "                    parts.append(item[\"text\"])\n",
        "                elif \"text\" in item:\n",
        "                    parts.append(str(item[\"text\"]))\n",
        "                else:\n",
        "                    parts.append(str(item))\n",
        "            else:\n",
        "                parts.append(str(item))\n",
        "        return \"\".join(parts).strip()\n",
        "\n",
        "    # 3) Fallback\n",
        "    return str(content).strip()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73790d1f",
      "metadata": {
        "id": "73790d1f"
      },
      "source": [
        "### 2.3. Tools that run spacific agent (with tools and without)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "6678099e",
      "metadata": {
        "id": "6678099e"
      },
      "outputs": [],
      "source": [
        "\n",
        "async def ask_agent_with_tools(prompt: str) -> str:\n",
        "    \"\"\"Runs the agent with access to tools.\"\"\"\n",
        "    if prompt is None:\n",
        "        raise ValueError(\"ask_agent_with_tools received prompt=None\")\n",
        "    tools = [solve_search_with_impl]\n",
        "    results, _, _ = await run_agent(prompt, tools, \"You MUST use the tool to compute the exact solution cost.\")\n",
        "    return results\n",
        "\n",
        "async def ask_agent_without_tools(prompt: str) -> str:\n",
        "    \"\"\"Runs the agent without access to tools.\"\"\"\n",
        "    if prompt is None:\n",
        "        raise ValueError(\"ask_agent_without_tools received prompt=None\")\n",
        "    results, _, _ = await run_agent(prompt, [], \"You are NOT allowed to use any tools. Estimate the solution cost.\")\n",
        "    return results\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "async def judge_agent(result_with: str, result_without: str) -> str:\n",
        "    \"\"\"\n",
        "    Judge agent that compares the two results.\n",
        "    \"\"\"\n",
        "    judge_prompt = f\"\"\"You are a Research Supervisor evaluating two agents.\n",
        "\n",
        "Context:\n",
        "- Agent WITH implementation uses a formal search algorithm (A*) that guarantees a minimal solution cost under the defined operators.\n",
        "- Agent WITHOUT implementation provides an estimate based on reasoning only and does NOT guarantee optimality.\n",
        "\n",
        "Given:\n",
        "Agent WITH implementation returned: {result_with}\n",
        "Agent WITHOUT implementation returned: {result_without}\n",
        "\n",
        "Instructions:\n",
        "1. If the results differ, assume the algorithmic agent is more reliable unless there is explicit evidence of an implementation bug.\n",
        "2. Explain why algorithmic guarantees outweigh heuristic estimation in combinatorial optimization.\n",
        "3. Briefly summarize the implication of the discrepancy.\n",
        "\n",
        "Return a concise comparison and conclusion.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "    result, _, _ = await run_agent(\n",
        "        prompt=judge_prompt,\n",
        "        tools=[],\n",
        "        sys_msg=\"You are a Research Supervisor comparing two agents.\"\n",
        "    )\n",
        "    return result\n",
        "\n"
      ],
      "metadata": {
        "id": "DkPli0GRehGM"
      },
      "id": "DkPli0GRehGM",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f88f6ab0",
      "metadata": {
        "id": "f88f6ab0"
      },
      "source": [
        "## 3. Run the Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "6e1fbfba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e1fbfba",
        "outputId": "cd38a606-75c0-4850-d594-f1b6662197ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "With implementation: 6\n",
            "Without implementation: 10\n",
            "\n",
            "Judge:\n",
            "**Comparison and Conclusion:**\n",
            "\n",
            "The Agent WITH implementation returned a cost of 6, while the Agent WITHOUT implementation returned a cost of 10. Given that the Agent WITH implementation utilizes a formal search algorithm (A*) which guarantees optimality, its result of 6 is considered more reliable.\n",
            "\n",
            "**Explanation of Algorithmic Guarantees:**\n",
            "\n",
            "In combinatorial optimization problems, algorithmic guarantees are paramount. Formal search algorithms like A* systematically explore the solution space, guided by a well-defined cost function and heuristic. This systematic exploration, coupled with the guarantee of optimality (under correct implementation and problem definition), ensures that the algorithm will find the absolute best solution. In contrast, heuristic estimations, while often faster, rely on educated guesses and approximations. They do not guarantee that the found solution is the best possible; it might be a locally optimal solution or even significantly suboptimal. The discrepancy highlights this fundamental difference: the algorithmic agent found a demonstrably better solution because its method is designed to find the *best* solution, not just a *good* one.\n",
            "\n",
            "**Implication of Discrepancy:**\n",
            "\n",
            "The significant difference in results (6 vs. 10) implies that the heuristic estimation provided by the Agent WITHOUT implementation is considerably inaccurate and has led to a suboptimal solution. This suggests that relying solely on heuristic reasoning for this problem would have resulted in a significantly less efficient outcome.\n"
          ]
        }
      ],
      "source": [
        "# THE JUDGE AGENT RUNNER\n",
        "async def orchestrator_agent(user_input: str):\n",
        "    \"\"\"\n",
        "    Orchestrates the full flow:\n",
        "    1. Builds prompts from user input\n",
        "    2. Calls both agents\n",
        "    3. Calls the judge\n",
        "    \"\"\"\n",
        "\n",
        "    # Build the prompt once\n",
        "    base_prompt = f\"\"\"You are solving a deterministic puzzle.\n",
        "\n",
        "Rules:\n",
        "1. The state is a vertical stack of N cubes.\n",
        "2. Each cube has exactly two DIFFERENT colors:\n",
        "   - one visible (front)\n",
        "   - one hidden (back)\n",
        "3. The goal depends ONLY on the visible colors, from top to bottom.\n",
        "4. Allowed operations:\n",
        "   a) Spin: rotate ONE cube, swapping its visible and hidden colors. Cost = 1.\n",
        "   b) Flip: choose a cube and reverse the order of all cubes below it (including it).\n",
        "      The chosen cube remains in place. Cost = 1.\n",
        "5. You may use intermediate states.\n",
        "6. The objective is to reach the goal with MINIMUM total cost.\n",
        "\n",
        "Estimate the minimum solution cost.\n",
        "Return ONLY an integer.\n",
        "{user_input}\n",
        "\"\"\"\n",
        "\n",
        "    # Run agents\n",
        "    with_impl = await ask_agent_with_tools(base_prompt)\n",
        "    without_impl = await ask_agent_without_tools(base_prompt)\n",
        "\n",
        "    # Judge\n",
        "    final_decision = await judge_agent(with_impl, without_impl)\n",
        "\n",
        "    return {\n",
        "        \"with_implementation\": with_impl,\n",
        "        \"without_implementation\": without_impl,\n",
        "        \"judge\": final_decision\n",
        "    }\n",
        "\n",
        "user_input = \"\"\"\n",
        "Start: (10,1),(50,5),(40,4),(20,2),(3,30)\n",
        "Goal: 1,2,3,4,5\n",
        "\"\"\"\n",
        "\n",
        "result = await orchestrator_agent(user_input)\n",
        "\n",
        "print(\"With implementation:\", result[\"with_implementation\"])\n",
        "print(\"Without implementation:\", result[\"without_implementation\"])\n",
        "print(\"\\nJudge:\")\n",
        "print(result[\"judge\"])\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}