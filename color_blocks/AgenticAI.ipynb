{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88079a16",
   "metadata": {},
   "source": [
    "# Agentic AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105ae079",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e8d156",
   "metadata": {},
   "source": [
    "### Install a Local LLM with Ollama\n",
    "\n",
    "To run this project locally, we will install and use **Ollama**, a lightweight runtime for local large language models.\n",
    "\n",
    "**Download Ollama:**  \n",
    "https://ollama.com/\n",
    "\n",
    "Once installed, you can pull any model you want to run.  \n",
    "Below are a few recommended examples, but you are free to pick any size or model from the Ollama library.\n",
    "\n",
    "ollama pull qwen3:0.6b\n",
    "\n",
    "or\n",
    "\n",
    "ollama pull ibm/granite4:350m\n",
    "\n",
    "or\n",
    "\n",
    "Choose any model you prefer, make sure the model supports tools.\n",
    "Browse available models here:\n",
    "https://ollama.com/library\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42933887",
   "metadata": {},
   "source": [
    "### Python requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e73d984",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langgraph langchain-google-genai langchain-core mcp langchain-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182e2139",
   "metadata": {},
   "source": [
    "## 1. Define FastMCP Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3eeb67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcp.server.fastmcp import FastMCP\n",
    "import math\n",
    "\n",
    "# Initialize FastMCP\n",
    "mcp = FastMCP(\"Unified Solver\")\n",
    "\n",
    "@mcp.tool()\n",
    "def calculate_sum(a: float, b: float) -> float:\n",
    "    \"\"\"Calculates the sum of two numbers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "@mcp.tool()\n",
    "def calculate_power(base: float, exponent: float) -> float:\n",
    "    \"\"\"Calculates the power of a base number.\"\"\"\n",
    "    return math.pow(base, exponent)\n",
    "\n",
    "# TO DO: Add more tools as needed for your application\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc32e06d",
   "metadata": {},
   "source": [
    "## 2. LLM + MCP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6e9ea7",
   "metadata": {},
   "source": [
    "### 2.1. Global instance of our LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420c2293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "# Choose your model here, can be Ollama or Google Gemini\n",
    "# Can also switch between different model sizes as needed\n",
    "model = \"qwen3:0.6b\"\n",
    "model = \"ibm/granite4:350m\"\n",
    "global_llm = ChatOllama(model=model, temperature=0.0)\n",
    "\n",
    "# SETUP API KEY if using Google Gemini\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_GOOGLE_API_KEY_HERE\"\n",
    "\n",
    "# model = \"gemini-2.5-flash\"\n",
    "# model = \"gemini-2.5-flash-lite\"\n",
    "# global_llm = ChatGoogleGenerativeAI(model=model, temperature=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7d5dee",
   "metadata": {},
   "source": [
    "### 2.2. Our agent graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dba39da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState, START, StateGraph\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.checkpoint.memory import MemorySaver # Optional: For saving graph state\n",
    "\n",
    "\n",
    "def create_agent_graph(sys_msg, tools):\n",
    "    \"\"\" Creates a LangGraph StateGraph with the given tools integrated.\"\"\"\n",
    "\n",
    "    llm = global_llm\n",
    "\n",
    "    if tools:\n",
    "        llm_with_tools = llm.bind_tools(tools)\n",
    "    else:\n",
    "        llm_with_tools = llm\n",
    "\n",
    "    # Node\n",
    "    def assistant(state: MessagesState):\n",
    "        return {\n",
    "            \"messages\": [\n",
    "                llm_with_tools.invoke([sys_msg] + state[\"messages\"], think=False)\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    # Graph\n",
    "    builder = StateGraph(MessagesState)\n",
    "\n",
    "    # Define the basic graph structure\n",
    "    builder.add_node(\"assistant\", assistant)\n",
    "    builder.add_edge(START, \"assistant\")\n",
    "\n",
    "    if tools:\n",
    "        builder.add_node(\"tools\", ToolNode(tools))  \n",
    "        builder.add_conditional_edges(\n",
    "            \"assistant\",\n",
    "            tools_condition,\n",
    "        )\n",
    "        builder.add_edge(\"tools\", \"assistant\")\n",
    "\n",
    "    react_graph = builder.compile()\n",
    "\n",
    "    return react_graph\n",
    "\n",
    "\n",
    "async def run_agent(prompt, tools, sys_msg=\"\"):\n",
    "\n",
    "    sys_msg = SystemMessage(content=sys_msg)\n",
    "\n",
    "    # 3. Create Graph\n",
    "    graph = create_agent_graph(sys_msg, tools)\n",
    "    \n",
    "    # 4. Run (using ainvoke for async tools)\n",
    "    config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "    result = await graph.ainvoke({\"messages\": [HumanMessage(content=prompt)]}, config)\n",
    "\n",
    "    last_msg = result[\"messages\"][-1].content\n",
    "\n",
    "    # Extract tool names and outputs\n",
    "    tools_used = []\n",
    "    tools_output = []\n",
    "    \n",
    "    # Parsing logic specific to your request\n",
    "    for msg in result[\"messages\"]:\n",
    "        # In LangChain, tool calls are usually in 'tool_calls' attribute of AIMessage\n",
    "        # or 'name' attribute if it is a ToolMessage\n",
    "        if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "             for tool_call in msg.tool_calls:\n",
    "                tools_used.append(tool_call['name'])\n",
    "        \n",
    "        if msg.type == 'tool':\n",
    "            tools_output.append(msg.content)\n",
    "\n",
    "    return last_msg, tools_used, tools_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73790d1f",
   "metadata": {},
   "source": [
    "### 2.3. Tools that run spacific agent (with tools and without)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6678099e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@mcp.tool()\n",
    "async def ask_agent_with_tools(prompt) -> str:\n",
    "    \"\"\" Runs the agent with access to tools.\"\"\"\n",
    "    tools = [calculate_sum, calculate_power]\n",
    "    results = await run_agent(prompt, tools)\n",
    "    return results[0]\n",
    "\n",
    "@mcp.tool()\n",
    "async def ask_agent_without_tools(prompt) -> str:\n",
    "    \"\"\" Runs the agent without access to tools.\"\"\"\n",
    "    # return await run_agent(prompt, [])[0]\n",
    "    results = await run_agent(prompt, [])\n",
    "    return results[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88f6ab0",
   "metadata": {},
   "source": [
    "## 3. Run the Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e1fbfba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Both assistants agree that 5 raised to the power of 3 equals 125. The first assistant used a calculator, while the second assistant calculated it mentally. Both methods are equivalent for this calculation.\n",
      "Tools Used: ['ask_agent_with_tools', 'ask_agent_without_tools']\n",
      "Tool Outputs: ['The result of 5 raised to the power of 3 is 125.', 'The result of 5 raised to the power of 3 (5^3) is 125.']\n"
     ]
    }
   ],
   "source": [
    "# THE JUDGE AGENT RUNNER\n",
    "\n",
    "sys_msg = \"\"\"\n",
    "    You are a Research Supervisor. You have two assistants:\n",
    "    1. 'ask_agent_with_tools': A precise mathematician with a calculator.\n",
    "    2. 'ask_agent_without_tools': A theorist who estimates mentally.\n",
    "\n",
    "    Your Goal:\n",
    "    When a user asks a math question, you must:\n",
    "    First run ask_agent_with_tools for the answer.\n",
    "    Second run ask_agent_without_tools for the answer.\n",
    "    Third Compare their answers in your final response and explain any discrepancies.\n",
    "    \"\"\"\n",
    "\n",
    "prompt = \"Compare the two assistants for the question 'What is 5 to the power of 3?'\"\n",
    "\n",
    "tool_list = [ask_agent_with_tools, ask_agent_without_tools]\n",
    "\n",
    "response, tools, outputs = await run_agent(prompt, tool_list, sys_msg)\n",
    "print(f\"Response: {response}\")\n",
    "print(f\"Tools Used: {tools}\")\n",
    "print(f\"Tool Outputs: {outputs}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
